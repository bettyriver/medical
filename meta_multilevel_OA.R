
# =========================
# Multilevel meta-analysis for OA MRI metrics (CV example) 
# Author: generated by ChatGPT
# Requirements: packages metafor, clubSandwich, readxl, dplyr, tidyr, stringr
# =========================

# ---- 0) Packages ----
library(readr)
req <- c("metafor", "clubSandwich", "readxl", "dplyr", "tidyr", "stringr", "purrr","readr")
new <- setdiff(req, rownames(installed.packages()))
if (length(new)) install.packages(new)
invisible(lapply(req, library, character.only = TRUE))

# ---- 1) Load Excel ----
path_xlsx <- "Rtable_July18/Table2.xlsx"      # assumes it's in the working directory
if (!file.exists(path_xlsx)) {
  stop("Table2.xlsx not found. Put it in the working directory or update 'path_xlsx'.")
}

sheet_names <- readxl::excel_sheets(path_xlsx)
message("Sheets found: ", paste(sheet_names, collapse=", "))

# ---- 2) Helper functions ----

# Fisher transforms (for ICC/Kappa if needed later)
fisher_z      <- function(r) 0.5 * log((1 + r) / (1 - r))
inv_fisher_z  <- function(z) (exp(2*z) - 1) / (exp(2*z) + 1)

# Log CV transforms
log_cv        <- function(cv_percent) log(cv_percent / 100)
inv_log_cv    <- function(lcv) 100 * exp(lcv)

# SE on different scales
# Derive SE from CI (two-sided normal approx)
se_from_ci <- function(est, lcl, ucl, zcrit = 1.96) {
  ifelse(!is.na(lcl) & !is.na(ucl),
         (ucl - lcl) / (2 * zcrit),
         NA_real_)
}

# CV: (preferred) pool on log scale
# If only n is available, use var[log(CV)] ≈ 1/(2*(n-1))
var_logcv_from_n <- function(n) {
  ifelse(!is.na(n) & n > 1, 1 / (2 * (n - 1)), NA_real_)
}

# If SE for CV% is available, delta-method to log scale:
# Var[log(CV)] ≈ (SE(CV)/CV)^2
var_logcv_from_cv_and_se <- function(cv_percent, se_cv_percent) {
  ifelse(!is.na(cv_percent) & !is.na(se_cv_percent) & cv_percent > 0,
         (se_cv_percent / cv_percent)^2, NA_real_)
}

# ICC/Kappa: if SE(r) available, z_se = se_r / (1 - r^2); var_z = z_se^2
var_z_from_r_and_ser <- function(r, se_r) {
  ifelse(!is.na(r) & !is.na(se_r) & abs(r) < 1,
         (se_r / (1 - r^2))^2, NA_real_)
}

# ICC: fallback SE when only n and number of measurements (m) and r are known (Bonett-style large-sample)
se_icc_from_n_m_r <- function(n, m, r) {
  ifelse(!is.na(n) & !is.na(m) & !is.na(r) & n > 1 & m > 1,
         sqrt(2 * (1 - r)^2 * (1 + (m - 1) * r)^2 / (m * (n - 1) * (m - 1))),
         NA_real_)
}

# Kappa fallback: se ≈ (1 - kappa^2)/sqrt(n)
se_kappa_from_n_r <- function(n, r) {
  ifelse(!is.na(n) & !is.na(r) & n > 0,
         (1 - r^2) / sqrt(n), NA_real_)
}

# SRM: se ≈ sqrt( 1/n + srm^2 / (2n) )
se_srm_from_n_val <- function(n, srm) {
  ifelse(!is.na(n) & !is.na(srm) & n > 0,
         sqrt(1/n + (srm^2) / (2*n)),
         NA_real_)
}

# Build yi/vi from generic columns
se_from_sd_n <- function(sd, n) ifelse(!is.na(sd) & !is.na(n) & n > 0, sd / sqrt(n), NA_real_)

# ---- 3) Read CV data from your sheets and build effect sizes ----
# We expect columns: Number (study id), CV_sample_size, CV_value, CV_LCL, CV_UCL, CV_SD, CV_SE
# We'll create: study, eff (row id), yi = log(CV/100), vi (variance of yi)

read_cv_sheet <- function(sheet) {
  raw <- readxl::read_excel(path_xlsx, sheet = sheet, guess_max = 5000)

  # Normalize names
  names(raw) <- gsub("\\s+", "_", names(raw))

  # Helper: robust parser that works for character OR numeric OR factor
  as_num <- function(x) suppressWarnings(readr::parse_number(as.character(x)))

  df <- raw %>%
    dplyr::rename(
      study    = dplyr::any_of(c("Number", "Study", "Study_ID")),
      n_cv     = dplyr::any_of(c("CV_sample_size", "CV_N", "N_CV", "n_CV")),
      cv_value = dplyr::any_of(c("CV_value","CV","cv","Cv")),
      cv_lcl   = dplyr::any_of(c("CV_LCL","CV_Lowerlimits","CV_lower","CV_Low")),
      cv_ucl   = dplyr::any_of(c("CV_UCL","CV_upperlimits","CV_upper","CV_High")),
      cv_sd    = dplyr::any_of(c("CV_SD","SD_CV","sd_cv")),
      cv_se    = dplyr::any_of(c("CV_SE","SE_CV","se_cv"))
    ) %>%
    mutate(
      # keep study as string ID (e.g., Roman numerals like Ⅳ24 are fine)
      study    = as.character(study),
      n_cv     = as_num(n_cv),
      cv_value = as_num(cv_value),
      cv_lcl   = as_num(cv_lcl),
      cv_ucl   = as_num(cv_ucl),
      cv_sd    = as_num(cv_sd),
      cv_se    = as_num(cv_se),
      sheet    = sheet,
      eff      = dplyr::row_number()
    )

  df
}



# Which sheets clearly contain CV data in your file
cv_sheets <- intersect(sheet_names, c("1cartilage","2Bone","3 Denuded area"))
if (length(cv_sheets) == 0) {
  stop("Could not find expected CV sheets (1cartilage, 2Bone, 3 Denuded area).")
}

cv_raw <- purrr::map_dfr(cv_sheets, read_cv_sheet)

# Compute yi and vi on log scale
cv_dat <- cv_raw %>%
  mutate(
    # effect size: log(CV/100)
    yi = log_cv(cv_value),
    # try to get sd from CI width first (if cv_se missing):
    se_from_ci_percent = se_from_ci(cv_value, cv_lcl, cv_ucl),
    # best available SE on % scale
    se_cv_percent = dplyr::coalesce(cv_se, se_from_ci_percent, se_from_sd_n(cv_sd, n_cv)),
    # variance on log scale (primary choice: from cv & se_cv via delta method)
    vi1 = var_logcv_from_cv_and_se(cv_value, se_cv_percent),
    # fallback variance if we only have n
    vi2 = var_logcv_from_n(n_cv),
    vi  = dplyr::coalesce(vi1, vi2)
  ) %>%
  # drop rows without yi or vi
  filter(is.finite(yi), is.finite(vi)) %>%
  mutate(
    study = as.factor(study),
    eff   = as.factor(paste(sheet, eff, sep=":")),
    tissue = dplyr::case_when(
      sheet == "1cartilage" ~ "cartilage",
      sheet == "2Bone" ~ "bone",
      sheet == "3 Denuded area" ~ "denuded_area",
      TRUE ~ as.character(sheet)
    )
  )

# Sanity
if (nrow(cv_dat) < 2) stop("Not enough CV rows with yi/vi to meta-analyze.")

# ---- 4) Multilevel REML model: effects nested within study ----
fit_cv <- metafor::rma.mv(yi, vi, random = ~ 1 | study/eff, method = "REML", data = cv_dat)
print(fit_cv)
cat("\n--- Cluster-robust (CR2) Wald test (optional) ---\n")
print(clubSandwich::coef_test(fit_cv, vcov = "CR2", cluster = cv_dat$study))

# Predictions (overall pooled log-CV) + back-transform to %
pred_cv <- predict(fit_cv)  # includes ci.lb / ci.ub in log space
pooled_cv_percent <- inv_log_cv(pred_cv$pred)
lcl_cv_percent    <- inv_log_cv(pred_cv$ci.lb)
ucl_cv_percent    <- inv_log_cv(pred_cv$ci.ub)

cat("\nPooled CV (%) = ", round(pooled_cv_percent, 3),
    "\n95% CI (%)   = (", round(lcl_cv_percent, 3), ", ", round(ucl_cv_percent, 3), ")\n", sep="")

# ---- 5) Optional: by-tissue models for CV ----
by_tissue <- split(cv_dat, cv_dat$tissue)
fits_by_tissue <- lapply(by_tissue, function(dd) {
  if (nrow(dd) >= 2) {
    metafor::rma.mv(yi, vi, random = ~ 1 | study/eff, method = "REML", data = dd)
  } else {
    NULL
  }
})

# summarize by tissue
cv_tissue_summary <- purrr::imap_dfr(fits_by_tissue, function(fit, tiss) {
  if (is.null(fit)) return(NULL)
  pr <- predict(fit)
  data.frame(
    tissue = tiss,
    pooled_cv_percent = inv_log_cv(pr$pred),
    lcl_percent = inv_log_cv(pr$ci.lb),
    ucl_percent = inv_log_cv(pr$ci.ub),
    k_effects = fit$k,
    k_studies = length(unique(fit$X$study)),
    tau2_components = paste(round(fit$sigma2, 6), collapse=";")
  )
})

# ---- 6) Export tidy outputs ----
overall_out <- data.frame(
  pooled_cv_percent = pooled_cv_percent,
  lcl_percent = lcl_cv_percent,
  ucl_percent = ucl_cv_percent,
  k_effects = fit_cv$k,
  k_studies = length(unique(fit_cv$X$study)),
  tau2_components = paste(round(fit_cv$sigma2, 6), collapse=";")
)

write.csv(overall_out, "cv_overall_multilevel.csv", row.names = FALSE)
if (nrow(cv_tissue_summary)) write.csv(cv_tissue_summary, "cv_by_tissue_multilevel.csv", row.names = FALSE)

# ---- 7) Skeleton functions for ICC/Kappa/SRM (fill once those tabs are provided) ----

# Given a data frame with columns:
# study, eff, n, r (ICC or Kappa), se_r (optional), m (Number_of_measurements, optional)
build_icc_kappa_dataset <- function(df, effect = c("ICC","Kappa")) {
  effect <- match.arg(effect)
  r <- df$r
  r[r >= 1] <- 0.999  # avoid boundary issues
  z <- fisher_z(r)
  # derive var in z-space
  v1 <- var_z_from_r_and_ser(r, df$se_r)
  if (effect == "ICC") {
    v2 <- se_icc_from_n_m_r(df$n, df$m, r)^2
  } else {
    v2 <- se_kappa_from_n_r(df$n, r)^2
  }
  vi <- coalesce(v1, v2)
  out <- df %>% mutate(yi = z, vi = vi)
  out
}

# SRM: columns: study, eff, n, srm, se_srm (optional)
build_srm_dataset <- function(df) {
  v1 <- ifelse(!is.na(df$se_srm), df$se_srm^2, NA_real_)
  v2 <- se_srm_from_n_val(df$n, df$srm)^2
  vi <- coalesce(v1, v2)
  out <- df %>% mutate(yi = srm, vi = vi)
  out
}

# Example model call for generic dataset with yi/vi:
# fit <- rma.mv(yi, vi, random = ~ 1 | study/eff, method="REML", data = mydat)
# summary(fit); predict(fit)

message("\nDone. Outputs written: cv_overall_multilevel.csv, cv_by_tissue_multilevel.csv\n")
